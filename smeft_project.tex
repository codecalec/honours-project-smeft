\documentclass[a4paper,11pt]{article}

\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[style=numeric-comp,sorting=none]{biblatex}
\usepackage{warning} % for warning messages

\parskip=1.5ex
\parindent=0pt

\usepackage{url}
\usepackage{microtype}
\usepackage{lmodern}
\usepackage[colorlinks=true, citecolor=red, linkcolor=blue]{hyperref}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[affil-it]{authblk}


\usepackage{todonotes}

\usepackage{siunitx}

\usepackage[compat=1.1.0]{tikz-feynman} % generate feynman diagrams

\renewcommand{\vec}{\mathbf}
\newcommand{\ts}{\textsuperscript}

\bibliography{references}
\overfullrule=2cm % allows to find overfull hboxes much quicker

\binoppenalty=3000
\relpenalty=3000
\clubpenalty=10000
\widowpenalty=10000

\title{Constraining the Standard Model Effective Field Theory with 2D differential cross section of top quark pair production}
\author{Alexander Veltman\\{\small Advisor: Dr.\ James Keaveney}}
\affil{Department of Physics,\\University of Cape Town}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}
\newpage
\section{Introduction}

The Standard Model (SM) is the best description of nature at the subatomic level that modern physics has to offer.
It describes a collection of particles and the forces between them which are mediated by other particles.
The predictions of physical observables made by the SM can be verified to extreme precision using the latest measurements taken from particle colliders such as the LHC at CERN.
However, there is still some phenomena which are not explained in the SM.
It does not provide an explanation for dark energy and dark matter, for electroweak symmetry breaking or for the Gauge hierarchy problem~\cite{morrissey2012}..
Though the quantity of data available to physicists are orders of magnitudes greater than years prior to the construction of the LHC, no direct searchs for beyond Standard Model (BSM) phenomena have found conclusive evidence of BSM physics at the energies scale of the LHC.

The lack of observations of new physics could be the result of two possible options.
There is the possibility that new physics which may describe the short-comings of the SM exists but is very weakly coupled to the SM particles.
This would make identifying this physics incredibly difficult with the current methods with which particle physics is measured.
The other option suggests that this new physics occurs at energies far larger than what is achieved within the most advanced particle collider, the LHC.
If the SM is considered to be an effective field theory, it is expected that this heavier physics will result in effective interactions which could be identified at the current energy level of the LHC.
This is the motivation of what is known as the Standard Model effective field theory (SMEFT).

SMEFT is a theoretical framework which introduces a set of higher dimensional operators into the SM with a coupling constant for each operator.
These operators describe possible effective interactions induced by heavier physics and are suppressed by the energy scale squared of the new physics.
Due to the increase in detailed measurements of processes such as top pair production, it is possible to use LHC data in order to estimate the couplings for the SMEFT operators.

This report applies the framework of SMEFT to a two sets of top quark observables from the ATLAS experiment at CERN.
This involves a comparison of single and double differential cross section top pair data in order to determine their ability to constrain top quark related SMEFT operators.
The following report is outlined as follows.
It begins with some contextual information regarding the top physics in section~\ref{sec:topquarks}, an brief overview of the ATLAS detector in section~\ref{sec:ATLAS} and a more detailed explanation of SMEFT in section~\ref{sec:smeft}.
This will lead into section~\ref{sec:deft} in which a description of the SMEFT analysis package dEFT is provided.
The analysis beginning in section~\ref{sec:analysis} will describe the ATLAS data used in the report and will then be proceeded by the details of the Monte Carlo event generation required.
The analysis was performed for both a single top related operator in section~\ref{sec:analysis_one_op} and then using two operators in section~\ref{sec:analysis_two_op}.

\section{Top Physics}\label{sec:topquarks}

The top quark is the most massive particle in the Standard Model with a mass of $m_{t} =172.76\pm0.30$GeV~\cite{ParticleDataGroup:2020ssz} and is commonly used as an area to look for physics beyond the Standard Model.
It participates in both electroweak and strong interactions.

At the LHC, top quarks are usually produced in top-antitop pairs through the $gg\rightarrow t\bar{t}$ and $q\bar{q}\rightarrow t\bar{t}$ processes at leading order.
Leading order (LO) and next-to-leading order (NLO) refer to the number of interactions required for the process to occur.
An example of a LO process is $gg\rightarrow t\bar{t}$ which involves the incoming gluons annihilating into a single gluon which then decays into a top quark pair with no additional particles being created or annihilated.
The Feynman diagrams for these LO processes can be seen in Figure~\ref{fig:ttbar_production}.
It shows that no additional particles other than a single mediating particle and the initial and final states are present in the diagrams.
At the centre of mass energy of the $pp$ collision at the LHC, about 90\% of $t\bar{t}$ production occurs through $gg\rightarrow t\bar{t}$.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \feynmandiagram[horizontal=a to b] {
            i1 -- [gluon] a -- [gluon] i2,
            a -- [gluon] b,
            f1 [particle=$\bar{t}$] -- [fermion] b -- [fermion] f2 [particle=$t$],
        };
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \feynmandiagram[horizontal=a to b] {
            i1 [particle=$q$] -- [fermion] a -- [fermion] i2 [particle=$\bar{q}$],
            a -- b,
            f1 [particle=$\bar{t}$] -- [fermion] b -- [fermion] f2 [particle=$t$],
        };
    \end{subfigure}
    \caption{Lowest order Feynman diagrams for top quark pair production through $gg \rightarrow t\bar{t}$ and $q\bar{q} \rightarrow t\bar{t}$.}
    \label{fig:ttbar_production}
\end{figure}

\todo{ask james for sources and context}
Top quarks almost always decays into a $W$ boson and a $b$ quark since the top quark is unable to hadronise.
Due to the $t$ having the largest mass of the quarks, it is actually the only quark which can decay in this manner.
Top quark pair decay is characterised is by the manner in which the two W bosons produced by each of the tops decay.
The tops are then classified as decaying leptonically if the $W$ decays into a lepton and an associated neutrino or hadronically if the $W$ decays into 2 quarks.
Any resulting quarks in the final state hardonise to form jets of hadrons.
Due to high luminosity achieved by the LHC, detailed cross sectional measurements has allowed for the implementation of differential cross section measurements of top quark production into analyses.
This provides differential cross sectional information with respect to the change of an observable as opposed to integrated cross section which is the total cross section.



\nocite{Thomson:2013zua}
The interest in the top quark in terms of beyond standard model physics is due to it being the quark with the largest coupling to the Higgs boson with a Yukawa coupling $y_{t} \approx 1$.
This implies that the top is a major contributor to the first order correction of the Higgs mass which is related to the hierarchy problem.
Deviations of the top coupling or new top interactions are predicted by many BSM theories.
Since the birth of the LHC, the large quantities of top related measurements have allowed for incredible precision which has made the search for this exotic physics possible.

\section{LHC and the ATLAS experiment}\label{sec:ATLAS}
The Large Hadron Collider (LHC) is a two-ring-superconducting particle accelerator located at the European Organization for Nuclear Research (CERN) and is the centre-piece of proton-proton collision physics.
The design consists of two counter-rotating proton beams in a circular ring with a diameter of \SI{26.7}{\kilo\metre}.
During the years of 2016 to 2018, the maximum achievable centre-of-mass energy $\sqrt{s}$ at the LHC was \SI{13}{\tera\electronvolt} with a luminosity of \si{10^{34}\centi\metre\squared\per\second}.
Upgrades have been performed in recent years to allow energies of \SI{14}{\tera\electronvolt}.~\nocite{Evans:2008zzb}
These energies are the highest to be achieved for a particle collider and has allowed for notable landmarks in particle physics including the discovery and verification of the Higgs Boson.
The LHC is home to a family of detector experiments including ATLAS, ALICE, CMS and LHCb.

\nocite{Atlas2008}
The ATLAS collaboration is a large collaboration of physics, engineers, technicians and students with the goal of probing proton-proton collisions using the ATLAS detector.
It is considered, along with the CMS detector, as a general purpose detector which can measure the various different products created during the collisions.
The ATLAS detector consists of various components which together allow for the general detection of particle collisions.
The largest of which is the magnet system.
The magnet system allows for the adjustment of the trajectories of charged particle in order for the momenta to be measured within the subdetectors.
The Inner Detector (ID) is the component closest to the beam and cylindrically surrounds the beam pipe.
This detector performs momentum and vertex measurements as well as electron identification by applying subdetectors.
Further out from the beam, there are the calorimeters which provide electromagnetic and hadronic energy measurements.
These allow for the measurement and identification of photons, electrons, jets and missing transverse energy.
The muon spectrometer forms the outer part of the detector.
High muon momentum resolution is possible due to the three layers of high precision tracking chambers.
The LUCID detector was the first implemented and it is designed to be a luminosity monitor for ATLAS.
The Zero-Degree Calorimeter (ZDC) detects forward neutrons in heavy-ion collisions.
The ALFA (absolute luminosity for ATLAS) detector measures elastic proton-proton scattering.
All these components allow for the reconstruction and measurement of the various underlying physical processes.
\todo{what more could be added here}

\section{Standard Model Effective field theory}\label{sec:smeft}

\nocite{Zhang2011}
\nocite{franzosi2015probing}

Standard Model effective field theory (SMEFT) is a model-independent framework which systematically includes new effective particles and interactions into the Standard Model.
This is done by considering that some higher energy particles and interactions may exist and are described by some effective operators.
The framework introduces a set of higher-dimensional operator into the Standard Model Lagrangian which only contains operators of dimension-four.
These includes 59 independent six-dimensional operators (according to what is known as the Warsaw basis) which are built from Standard Model fields and follow the gauge symmetries of the Standard Model \cite{Grzadkowski_2010}.

With these additional operators, the SMEFT Lagrangian is defined as
\begin{equation}\label{eq:smeft_lagrangian}
    \mathcal{L}_{\text{SMEFT}} = \mathcal{L}_{\text{SM}} + \frac{1}{\Lambda^2} \sum\limits_{i} C_{i} O_{i} + \mathcal{O}\left(\frac{1}{\Lambda^3}\right)
\end{equation}
where $\mathcal{L}_{\text{SM}}$ is the Standard Model Lagrangian, $O_{i}$ is a dimension-six operator and $C_{i}$ is an associated dimensionless coupling constant known as a \emph{Wilson Coefficient}.
The operators are reduced by the energy scale $\Lambda$ of the proposed BSM physics.
Terms of order $\Lambda^{-3}$ are considered sufficiently suppressed and operators of dimensions greater than six are removed since they would appear in these terms.
The calculation of the cross section of a process~\cite{Hartland_2019} in the SMEFT is
\begin{equation}\label{eq:smeft_cross_section}
    \sigma_{\text{SMEFT}} = \sigma_{\text{SM}} + \sum\limits_{i} \frac{1}{\Lambda^2} C_{i} \sigma_{i} + \sum\limits_{j,k} \frac{1}{\Lambda^4} C_{j} C_{k} \sigma_{j k}
\end{equation}
where $\sigma_{\text{SM}}$ is the cross section in the SM. This can also be extended to differential cross sections which are commonly obtained in high energy physics experiments.
It is important to note that when $C_{i}=0$ for all operators, the SMEFT Lagrangians simplifies to the SM Lagrangian.
This implies that a measurement of a Wilson coefficient which deviates from zero would indicate some effective interaction occurring.

The differential cross section of a process with respect to an observable $X$ in the SMEFT is described by
\begin{equation}\label{eq:smeft_diff_cross_section}
    \frac{d\sigma_{\text{SMEFT}}}{dX} = \frac{d\sigma_{\text{SM}}}{dX} + \sum\limits_{i} \frac{1}{\Lambda^2} C_{i} \frac{d\sigma_{i}}{dX} + \sum\limits_{j,k} \frac{1}{\Lambda^4} C_{j} C_{k} \frac{d\sigma_{j k}}{dX}
\end{equation}
where $X$ is a dependent of the cross section which include observables like a polar angle of a final state particle or the mass of the final state.
These differential cross sections are commonly represented in data by binned measurements .
Using (\ref{eq:smeft_diff_cross_section}), the influences of SMEFT can be identified within differential cross section measurements obtained through modern collider experiments.
There also exist global fits which involve using multiple different independent observables to constrain multiple Wilson coefficients.
This allows different operators which are related to some of the observables to be more effectively constrained.

There has been interest in looking at the influences of SMEFT within the study of top quarks~\cite{Hartland_2019,Buckley_2015,Brivio_2020} due to the possibility of the top quark as an area for possible BSM physics.
This report will investigate top pair production whose cross section at leading order is only impacted by limited sets of the dimension-6 operators. For the $gg \rightarrow t\bar{t}$ process, the only relevant operator is the chromomagnetic moment operator, $O_{tg}$.
The $q\bar{q} \rightarrow t\bar{t}$ process is affected by $O_{tg}$ as well as a set of 8 operators known as four-fermion operators.
This report will only include the four-fermion operator $O_{tq}^{8}$.

The explicit form of these operators are
\begin{align*}
    O_{tG} = (\bar{Q} \sigma^{\mu \nu} T^{A} t) \tilde{\phi} G^{A}_{\nu \mu} && O_{tq}^{8} = (\bar{q}_{i} \gamma^{\mu} T^{A} q_{i})(\bar{t} \gamma_{\mu} T^{A} t)
\end{align*}
where these some of the interactions which will be present in $t\bar{t}$ production can be seen in the terms.
The operator $O_{tG}$ introduces or modifies the interactions seen in Figure~\ref{fig:smeft_diagrams} and the four fermion operator $O_{tq}^{8}$ introduces an effective contact vertex of $q\bar{q} \rightarrow t\bar{t}$.
Some of these are modifications of SM vertices while some are completely new effective interactions which are not present in the SM.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \feynmandiagram [small, horizontal=a to b] {
            i1 [particle=$\bar{q}$] -- [fermion] a -- [gluon] b [dot] -- [fermion] f1 [particle=$t$],
            i2 [particle=$q$] -- [anti fermion] a,
            b   -- [anti fermion] f2 [particle=$\bar{t}$],
        };
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \feynmandiagram [small, vertical=a to b] {
            i1 [particle=$g$] -- [gluon] a [dot] -- [fermion] f1 [particle=$t$],
            i2 [particle=$g$] -- [gluon] b [dot] -- [anti fermion] f2 [particle=$\bar{t}$],
            b -- [fermion] a,
        };
    \end{subfigure}
    %\hfill
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{feynman}
                \vertex (a);
                \vertex [above left=1cm of a] (i1) {$g$};
                \vertex [above right=1cm of a] (f1) {$t$};
                \vertex [below left=1cm of a] (i2) {$g$};
                \vertex [below right=1cm of a] (f2) {$\bar{t}$};
            \diagram* {
                (i1) -- [gluon] (a) -- [fermion] (f1),
                (i2) -- [gluon] (a) -- [anti fermion] (f2),
            };
            \end{feynman}
        \end{tikzpicture}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{feynman}
                \vertex (a);
                \vertex [above left=1cm of a] (i1) {$q$};
                \vertex [above right=1cm of a] (f1) {$t$};
                \vertex [below left=1cm of a] (i2) {$\bar{q}$};
                \vertex [below right=1cm of a] (f2) {$\bar{t}$};
            \diagram* {
                (i1) -- [fermion] (a) -- [fermion] (f1),
                (i2) -- [anti fermion] (a) -- [anti fermion] (f2),
            };
            \end{feynman}
        \end{tikzpicture}
    \end{subfigure}
    \caption{Examples of leading order SMEFT diagrams which contribute to top pair production in SMEFT. The dotted vertices represents SM vertices which are modified by $O_{tg}$. The two rightmost diagrams are new interactions introduced by $O_{tg}$ and $O^{8}_{tq}$.}
    \label{fig:smeft_diagrams}
\end{figure}

\section{dEFT}\label{sec:deft}

dEFT or \emph{differential Effective Field Theory tool}, is an Python package created by Dr. James Keaveney~\cite{Keaveney_dEFT} to allow for predictions of SMEFT effects using differential cross section measurements.
The goal of dEFT is to build a predictive morphing model which can used to estimate a posterior distribution for the relevant Wilson coefficients using just a single configuration file containing both data and Monte Carlo predictions.
This allows for simple, fast and reproducible analyses which can be defined in a plain text file.
For this report, the repository was forked and further development was performed.
The version used for the analysis contained in this report is available through Github~\cite{codecalec_dEFT} or the PyPI repositories~\cite{pypi_dEFT}.
This section will provide an overview of dEFTs flavour of SMEFT analysis and how it can extract multi-dimensional probaility distribution functions for multilple Wilson coefficients from data.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{images/deft-workflow.pdf}
    \caption{Overview of dEFT workflow}
\end{figure}

\subsection{Model building}
dEFT creates predictions for the observables of interest for varying values of Wilson coefficients by constructing a linear morphing model.
The morphing model is a linear regression model which allows for the interpolation between different templates.
These templates are Monte Carlo predictions which are generated around the region of parameter space which would be relevant for some dataset.
For dEFT's application, cross sectional SMEFT predictions which are generated using a event generation framework are treated as templates.
These predictions must describe the relevant set of operators $O_{i}$ with varying Wilson Coefficients in order to produce a reliable model.
Using ($\ref{eq:smeft_cross_section}$), a linear model is constructed using these templates and produces a predictive model $\hat{\sigma}({C_i})$.
This model now allows for the prediction of some cross sectional observable for any values of Wilson coefficients pertaining to the relevant set of operators.
Due to the reliance on MC samples, the model is dependent on the ability to produce high quality SMEFT differential cross section predictions.

The model must be validated to ensure sensible predictions are being performed and the model predictions can be considered reliable.
Additional Monte Carlo samples are generated at coefficient values between the templates used to create the model.
These differential cross section samples are then compared to the model predictions to ensure the predictions agree within a suitable error comparable to the statistical error on the Monte Carlo samples.
Unfortunately, due to validating using Monte Carlo samples, this method of validation is still vulnerable to issues which may arise from the modelling of the samples themselves.

\subsection{Fitting Method}\label{sec:fitting}
Once a model has been built, a fit to data is possible.
dEFT performs fitting using Monte Carlo Markov Chain (MCMC) methods which allow for estimation of the likelihood distributions of the Wilson Coefficients using prior assumptions about their possible values.
The fitting procedure uses \emph{emcee}~\cite{Foreman_Mackey_2013} as its MCMC implementation.
MCMC requires an estimation for the likelihood function $P(y | C_{i})$ which represents the probability of obtaining some data $y$ given a set of model parameters $C_{i}$.

A common log likelihood definition for binned data with Gaussian errors with the associated model $f$ is
\begin{equation}\label{eq:likelihood}
    P(y | C_{i}) \propto \ln\mathcal{L}(y | C_{i}) = -\sum\limits_{n} (y_{n} - f_{n}(C_{i})) V^{-1} (y_{n} - f_{n}(C_{i}))
\end{equation}
where $y_{n}$ is the binned cross sectional data, $V$ is the associated covariance matrix and $f_{n}(C_{i})$ is the morphing model prediction.
In order for an estimation for the posterior likelihood distribution $P(C_{i} | y)$ to be made, a prior distribution $P(C_{i})$ is required.
This takes the form of uniform distributions defined by some minimum and maximum for each $C_{i}$ parameter.
MCMC will systematically sample throughout $C_{i}$ space building an estimation for the posterior distribution $P(C_{i}|y)$.
Properties regarding $C_{i}$ can then be inferred.

Since MCMC methods are used, an approximations for the $C_{i}$ distributions is obtained rather than a single value with an associated uncertainty which is common from other likelihood maximisation methods.
This avoids the issue of finding a local maximisation which can be common due to the quadratic nature of the SMEFT model.

The estimation for the coefficient is extracted from the likelihood distributions by considering percentiles of the discrete MCMC sampler prediction of the marginalised distribution of each operator.
The 50\ts{th} percentile is attributed as the estimation for the coefficient with the 16\ts{th} and 84\ts{th} percentile forming a 68\% confidence interval about the estimate.
This estimate allows for asymmetrical errors which can describe the measurement more faithfully than considering just the mean and variance of the samples.
\todo{maybe talk about other approaches for smeft analysis}

\section{Analysis}\label{sec:analysis}
This analysis examines the benefits of using double differential cross section measurements as a function of two observables in a SMEFT analysis.
The results are compared to outcomes when considering just a single differential cross section.
The main aim of a SMEFT analysis is to place constraints onto the Wilson coefficients of the SMEFT operators in order to constrain the potential occurrences of new interactions or modifications to the SM interactions.
Double differential cross sections are of interest due to the possibility of simultaneously constraining multiple operators.
Some operators may present as varying modifications to cross section distributions when considering the differential with repect to different observables.

The only SMEFT operators considered were $O_{tG}$ and $O_{tq}^{8}$ with corresponding Wilson coefficients $C_{tG}$ and $C_{tq}^8$.
For a full phenomenological study, the entire set of relevant operators should be included but this would drastically increase the scale of computational processing and analysis.
The limited set of operators should still reveal the influences of changing from a single dimensional differential cross section to a multidimensional differential cross section.

This section will begin with the details of the ATLAS cross section measurements and the Monte Carlo event generation needed to create the morphing models for determining the constraints on the Wilson coefficients.
This will move into applying this model and obtaining estimations for the distribution of the coefficients for both the single observable and the double observable.

\subsection{Discussion of Datasets}
This report will use differential cross section data of top pair production from the ATLAS experiment~\cite{ATLAS:2019hxz} at the CERN Large Hadron Collider.
The data used is publicly available through HEPData~\cite{hepdata1750330}.
This data was produced from pp collisions performed at a centre-of-mass energy $\sqrt{s} = 13$TeV over the course of 2015 and 2016 with an integrated luminosity of 36.1fb$^{-1}$.
The $t\bar{t}$ final states are extracted from the $\ell$+jets channel in the resolved topology.
This final state contains a leptonically decay top with a hadronically decaying top.
Resolved topology implies that the decay products of the hadronically decaying top quark are angularly well separated.

The double differential cross section observable considered was the differential cross section as a function of the invariant mass of the $t\bar{t}$ system $m_{t\bar{t}}$ and the transverse momentum of the hadronically decaying top quark $p_{T}^{t}$.
These are the mass of the top-antitop system in its rest frame and the component of the hadronically decaying top's momentum travelling radially from the beam line in the collider.
For the comparison with a single observable, the differential cross section as a function of just $m_{t\bar{t}}$ was examined.
The differential cross section data for just $m_{t\bar{t}}$ may be referred to as the single or 1D obervable and the differential cross section data with respect to $m_{t\bar{t}}$ and $p_{T}^{t}$ may be referred to as the double or 2D observable.

The data provided contains the associated statistical and systematic uncertainties introduced during the measurements of the cross sections.
Multiple sources of systematic uncertainty are introduced in the process of measuring the $t\bar{t}$ systems.
This includes the uncertainty associated with the reconstruction of the lepton and the various jet products of the process.
There is also the uncertainty associated with the tagging of jets containing $b$-hadrons as well as accounting for possible misidentifications of jet flavour.
The uncertainty associated with the reconstruction of the missing transverse momentum $E_{T}^{\text{miss}}$ must also be accounted for.
Error in the modelling of the signal and background contributions and the Monte Carlo statistical error involved in creating these models are propagated.

Through HEPData, the covariance matrices associated with the differential cross section measurements are made available to the public.
The analysis was initially conducted with the full covariance matrix of each data set which was implemented into the likelihood definition as seen in (\ref{eq:likelihood}).
Unfortunately, this caused the likelihood definition to produce fits which described the data very poorly compared to if only the diagonals of the matrices were considered.
For the differential cross section data with respect to $m_{t\bar{t}}$, the fit produced with the full covariance matrix had a corresponding $\chi^2$ per degree of freedom of 12.60 while this was reduced to 0.53 when only considering just the diagonal elements.
The covariance matrices provided were examined and it was found that they indicated that the bins of the histogram were highly correlated for a dataset of this type.
The correlation matrix for the histogram is defined as
\begin{equation}
    \textrm{corr}(\vec{x}) = \textrm{diag}(V) V \textrm{diag}(V)
\end{equation}
where $\vec{x}$ are the bins of the histrogram and $V$ is the associated covariance matrix.
The correlation between bins in the single differential cross section histogram is shown as an example in Figure~\ref{fig:correlation}.
This was considered to be an error in the covariance matrix provided by HEPData and the covariance between different bins were ignored.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/correlation.png}
    \caption{Correlation matrix between $m_{t\bar{t}}$ bins of histogram for the differential cross section with respect to $m_{t\bar{t}}$ dataset}
    \label{fig:correlation}
\end{figure}

\subsection{Monte Carlo event generation}
In order to build the morphing model required to generate cross sectional predictions, simulated samples were required throughout the space of Wilson coefficients for the operators of interest.
These samples were generate using the MadGraph5\textunderscore aMC@NLO~\cite{Alwall_2014} framework which allows for the simulation of processes for a user-defined Lagrangian.
The SMEFTatNLO~\cite{degrande2020automated} FEYNRULES model implements SMEFT tree level and NLO processes into MadGraph5.
Though there is the capacity to perform predictions at next-to-leading order, these calculations are very recent and greatly increase the processing time required to produce the Monte Carlo predictions.
This remains an area of exploration for further analysis.
The simulations were performed to fixed order where only the desired observables of $m_{t\bar{t}}$ and ${p_{T}^{t}}$ are calculated and binned in the same binning arrangement as the ATLAS dataset of interest.

Separate sets of MC signal was required for the single observable and the double observables analyses.
Events were generated with 13 values of $C_{tg}$ ranging from  -4 to 4 and 13 values of $C_{tq}^{8}$ from -4 to 4.
All permutations of these values were used resulting in a total number of 169 samples to build the two operator model and 13 samples to build the single operator model.
The two operator model validation involved generating 100 validation samples at points different to those used for the building of the model and the single operator model used 10 samples for validation.
This simulation step imposes a great challenge when wanting to expand into using more SMEFT operators in the model.
The number of required MC samples to ensure consistent description of the parameter space increases exponentially with the number of operators.
This is a major factor in the decision to not include all top pair related 4-fermion operators in this report.
It would drastically increase the required wall time and the computational capacity.

In order for the simulated events to be comparable to the data taken from ATLAS, the uncertainty associated with the simulation must be estimated and the lowest order simulation must be scaled to be of the order of the measurements through a $k$-factor.
This process is detailed in the following subsections.

\subsubsection{Uncertainty due to simulation}\label{sec:scale_variance}

The simulation calculations conducted were required to obtain a statistical accuracy of 1\% for the prediction of the integrated cross section of top-antitop production.
This is handled by an configuration parameter in the MadGraph5 event generation.
This level was considered reliable since this error was minimal in comparison to the total uncertainty of the ATLAS cross section data but still able to be computed in a reasonable time frame.
An accuracy requirement on each bin was unavailable and would have allowed for a clearer comparison to the error associated with each bin of the data.

Theoretical uncertainties on the Monte Carlo samples also needed to be accounted for.
This was done by varying the renormalisation and factorisation scales by two for the upper scale and by a half for the lower scale.
The scale variance was calculated through MadGraph5 using the SMEFTatNLO model with all operators being suppress.
This provided a scale variance prediction for the Standard Model at NLO which could then be applied to SMEFT samples.
\todo{talk about error compared to validation value}

\subsubsection{Calculation of \texorpdfstring{$k$}{k}-factor}
Since the generated Monte Carlo samples only included LO processes, these predictions needed to be scaled to be comparable to the necessary data sets which includes processes of all orders.
It can be considered fairly accurate to compare NNLO predictions to actual cross sectional data so a method of scaling the current predictions to this level is required.
Due to the difficulty in calculating the $k$-factor for different combinations of Wilson coefficients values, the $k$-factor for the Standard Model prediction was used across the various SMEFT predictions.

A flat $k$-factor across the differential cross sections was attempted to bring the LO predictions to the scale of NNLO predictions using the proportions between tree level calculations of total cross section~\cite{Alwall_2014} and measurement of total cross section using the ATLAS detector.
This method failed due to some regions of the differential cross sections being poorly described at LO which caused a flat scaling to incorrectly describe the shape of the distributions at NNLO.
This is exemplified by the low $m_{t\bar{t}}$ and high $p_{T}^{t}$ region in the two observable data set, as seen in Figure~\ref{fig:kfactor}.
This is remedied by requiring a per-bin $k$-factor when scaling from LO to NLO but still using the flat factor to build up to NNLO.
The per-bin $k$-factor was found by comparing the Standard Model predictions of MadGraph5 at LO and NLO and applying this ratio to the Monte Carlo signal.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/k_factor.png}
    \caption{Comparison of estimations of absolute double differential $t\bar{t}$ cross section with respect to $m_{t\bar{t}}$ and $p_{T}^{t}$ to varying orders. The scaling from an LO approximation to an NLO approximation was determined using a per-bin method and the NNLO prediction was determined using a flat factor.}
    \label{fig:kfactor}
\end{figure}

The theoretical error associated with the calculated $k$-factor is fairly difficult to propagate through the model construction and should be dominated by the error associated with the data.
This may have consequences on bins which are not well described at LO though the scale variances in these regions should dominate the uncertainties introduced by the $k$-factor.


\subsubsection{Response to operators}\label{sec:response}

With a set of observables declared, Monte Carlo simulated events were used to examine the response of $t\bar{t}$ differential cross sections when the associated Wilson coefficients were varied in the theory.
Various theoretical predictions are shown in Figure~\ref{fig:comparison} for the double differential cross section histogram explored in this report.
Focussing on the response with respect to $C_{tG}$, there appears be a general normalisation effect on the majority of bins of the distribution.
This provides a clear interpretation of the operator but exposes some issues in estimating $C_{tG}$.
If $O_{tG}$ is related to a scaling of the distribution, any other contribution which scales the theoretical predictions may impact the estimation of $C_{tG}$.
This would include the $k$-factor used to ensure the leading order MC predictions are comparable to the data.
It is assumed that any effects due to not describing higher order terms or error in the $k$-factor estimation would affect the distribution uniformly.
This implies that the width of constraints determined for $C_{tG}$ would not be effected if the scale of the MC samples were adjusted.

The operator $C_{tq}^{8}$ has a more varied response across the distribution.
It appears that the effects of $O_{tq}^{8}$ are more prominent at the higher regions of transverse momentum of the hadronically decaying top $p_{T}^{t}$.
A major difference between the response in $C_{tq}^{8}$ and $C_{tG}$ is that an increasing $C_{tq}^{8}$ does not indicate an increase of cross section in a bin.
This non-uniformity in how the bins transform could lead to coefficient distributions which aren't necessarily Gaussian.
If these effects are considered when modelling, it is expected that $C_{tq}^{8}$ may experience multiple optimised values which will be seen as a distribution with multiple peaks.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/data_plot_ctg.png}
        \caption{Response to $C_{tG}$}
    \end{subfigure}

    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/data_plot_ctq.png}
        \caption{Response to $C_{tq}^{8}$}
    \end{subfigure}
    \caption{Comparison of simulated theoretical predictions of double differential cross section of $t\bar{t}$ production as a function of $m_{t\bar{t}}$ and $p_{t}^{T}$ for varying values of $C_{tG}$ and $C_{tq}^{8}$. The ATLAS measurements are also shown.}
    \label{fig:comparison}
\end{figure}

\subsection{Single operator model}\label{sec:analysis_one_op}

As a test of method, two model were constructed considering only the $O_{tG}$ operator and applied to the 1-dimensional and 2-dimensional differential cross section measurements.
Sensible results for this limited analysis will show the ability for dEFT to generate a successful model using the Monte Carlo samples.
It would expected that links in the predictions exist between the two datasets which share an influence from the differential cross section with respect to the mass of the top quark pairs.
A separate model was created to suit each of the datasets which then required separate sets of Monte Carlo samples in which the value of $C_{tG}$ was varied and the value of $C_{tq}^{8}$ was suppressed to the level of the SM.


\subsubsection{Model Validation}

The models used were created using Monte Carlo samples which were influenced by statistical uncertainty.
For the model to be considered suitable, the model should provide predictions of other Monte Carlo samples with a similar degree of certainty.
This deviation is expected to be comparable to the 1\% required accuracy for integrated cross section imposed on the event generation within MadGraph5.

10 validation points of varying $C_{tG}$ were used for the 1-dimensional cross section and the 2-dimensional models.
In order to compare the validation tests to the statistical simulation accuracy, the average relative residuals between the validation samples and the models' prediction for the associated coefficient values were considered.
This is just the summation of the relative residuals of each bin in each test divided by the total number of bins across all tests.
The standard deviation of this value should be in-line with the required accuracy.

An example of one of these tests can be seen in Figure~\ref{fig:test_example}. All predictions across the bins in the tests are within at least 5\% of the MC sample provided by MadGraph5.
For most of the tests, the deviations per bin do exceed the 1\% accuracy required for the integrated cross section on some of the bins.
However, considering the average relative residuals allows for a more general estimation of the performance of the models predictive capacity.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\textwidth]{plots/validation_ATLAS-ctg_2_1D_1OP.png}
    \caption{Example of validation test for single observable model for $C_{tG}=2$. The error bars shown represent a 1\% deviation on the MadGraph5 prediction.}
    \label{fig:test_example}
\end{figure}

The average relative residuals with the associated standard deviation is $0.10\pm1.21\%$ for the 1D observable model and $0.35\pm1.30\%$ for the 2D observable model.
The results of the validation tests show that both models on average provide very small deviation compared to the Monte Carlo signal and is comparable to the uncertainty introduced by the statistical error associated with the Monte Carlo signal.

\subsubsection{Uncertainty due to scale variance}
The theoretical uncertainty related to the Monte Carlo samples are encapsulated by the upper and lower bound of the scale variance as mentioned in Section~\ref{sec:scale_variance}.
This uncertainty is propagated through the model by reperforming the fitting procedure for when the Monte Carlo samples are adjusted to the upper and lower scales.
This will provide two sets of results for the coefficients which can be used as bounds to estimate the systematic uncertainty introduced by the theoretical variance in the simulated samples.

The estimated uncertainty associated with the scale variance of the Monte Carlo samples for the 1-dimensional model is $^{+0.07}_{-0.06}$ and for the 2-dimensional model is $^{+0.16}_{-0.16}$.

\subsubsection{Results}
The estimated likelihood distributions created during the fitting procedure for the 1-dimensional observable and the 2-dimensional observable are shown in Figure~\ref{fig:corner_1OP}.
Both observable types produced likelihood distributions with a similar Gaussian shape.
The estimation for the Wilson coefficient using the single differential cross section was $C_{tG}=0.46^{+0.08}_{-0.09}(stat.)^{+0.07}_{-0.06}(sys.)$ and using the double differential cross section, the obtained estimate was $C_{tG}=0.47^{+0.05}_{-0.05}(stat.)^{+0.16}_{-0.16}(sys.)$.
The resulting prediction for the single and double differential cross section models is shown in Figure~\ref{fig:model_result_1OP}.
These predictions show that the optimised estimate for $C_{tG}$ provides better prediction of the data for both models then the SM prediction of the models.
The $\chi^{2}$ per degree of freedom statistic for these predictions with respect to their data quantify the goodness of fit.
The 1D observable model predictions has an associated $\chi^{2}$ per degree of freedom of 1.2 for the optimised prediction and 3.16 for the SM prediction.
For the 2D observable model, the $\chi^{2}$ per degree of freedom was 2.53 for the optimised prediction and 5.60 for the SM prediction.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg_1D_1OP.png}
        \caption{1-Dimensional Observable}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg_2D_1OP.png}
        \caption{2-Dimensional Observable}
    \end{subfigure}
    \caption{Estimation for the likelihood distribution for the Wilson coefficient $C_{tG}$. The dotted lines on distributions represent a 68\% confidence interval. Uncertainty estimates for the Wilson coefficients are discussed in Section \ref{sec:fitting} and only represent statistical error.}
    \label{fig:corner_1OP}
\end{figure}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_1D_1OP.png}
        \caption{1-Dimensional observable model}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_2D_1OP.png}
        \caption{2-Dimensional observable model}
    \end{subfigure}
    \caption{Predictions of $m_{t\bar{t}}$ differential cross section measurements using two models at the optimised value of the model parameter with respect to the respective ATLAS datasets. The data used to perform the fit and the models' predictions for when all SMEFT operators supressed are also shown.}
    \label{fig:model_result_1OP}
\end{figure}


The estimation for the optimised value of $C_{tG}$ for the models agree within a 68\% confidence interval and provides some interesting points of comparison.
The statistical uncertainty associated with the measurement is lower for the double differential cross section model.
This means that even though there are increased relative errors on the bins for double differential cross sections measurements then for single differential cross sections, the double observable model provides greater constraints on $C_{tG}$ than the single observable model.
This may indicate that using this double observable would allow for better constraints on Wilson coefficients in SMEFT analyses.
A caveat arises when the systematic uncertainty is considered.
The systematic uncertainty is larger when using the double observable which may indicate that the bins which are allowing for the greater constraint of the coefficient may be more affected by the scale variance.
This indicates that this increase in the ability to constrain the coefficient may expose regions which are not well modelled by the theoretical predictions.


\subsection{Double operator model}\label{sec:analysis_two_op}

The software and techniques for the single operator analysis could then be used to introduce another operator $O_{tq}^{8}$ as well as $O_{tG}$.
This additional operator increases the scale of the analysis since more Monte Carlo samples are required to sufficiently describe this now 2-dimensional parameter space of the models.
The estimation of likelihood distributions of the coefficients produced by the MCMC methods are now 2-dimensional and expose any correlations between the fitted parameters.

\subsubsection{Model Validation}

The results of the validation testing for both models can be seen in Figure~\ref{fig:residuals_hist} where deviations between the model predictions and the validation samples are shown by the average relative residuals.
The tests for the single observable model appear to follow an accuracy of 1.09\% which is in-line with the expectation from the event generation.
The average relative residuals for the double observable model appear to be distributed by a normal distribution as well but with a deviation of 1.68\%.
This result is larger than the validation deviation of the single obervable but is still comparable to the statistical accuracy of the MC samples.
When considering the results across all validation tests, both models appeared to provide sufficient agreement with the MadGraph5 samples and were used for estimating the distributions of the Wilson coefficients.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./plots/residuals_hist_1D.png}
        \caption{1-Dimensional Observable}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./plots/residuals_hist_2D.png}
        \caption{2-Dimensional Observable}
    \end{subfigure}
    \caption{Distribution of average relative residuals between model predictions at some set of coefficient values and the corresponding MC validation sample for each model. This includes the validation tests over 100 different validation points. The dotted line represents a Gaussian fit to the histogram through $\chi^2$ minimisation with the results of the fit shown.}
    \label{fig:residuals_hist}
\end{figure}

\todo{maybe include example validation plots}
\subsubsection{Results}

The estimations for the 2D likelihood distributions for the Wilson coefficients generated by each model are shown in Figure~\ref{fig:corner_2OP}.
It should be noted that the expansion of the parameter space to 2 operators has allowed for the description of the distribution in the form of a 2D likelihood distribution as well as a marginal distribution for each coefficient.
First, the results of the single observable analysis using two operators will be detailed.

\paragraph{Single observable model}

The distribution created using the single observable model provided estimates of $C_{tG}=0.24^{+0.11}_{-0.11}$ and $C_{tq}^{8}=1.56^{+0.25}_{-0.31}$ with these estimates only considering statistical deviation of the distribution.
A notable features of the marginal $C_{tq}^{8}$ distribution is a large peak at $C_{tq}^{8}\approx-2.5$ second less prominent peak located at $C_{tq}^{8}\approx-2.5$.
This is a product of the quadratic nature of the SMEFT model and is a behaviour which is expected when considering the response of the differential cross section with respect to $m_{t\bar{t}}$ as discussed in section~\ref{sec:response}.
The major peak in the distribution appears to show a negative correlation between $C_{tg}$ and $C_{tq}^8$.
This could be a result of the overall scaling introduced by $O_{tg}$ correlating with the scaling on some bins caused $O_{tq}^{8}$.

The model created using the upper bound of the scale variance of the MC signal produced an estimation of $C_{tG}=0.21$ and $C_{tq}=1.33$.
The lower bound of the scale variance of the MC signal produced an estimation of $C_{tG}=0.27$ and $C_{tq}=1.78$.
\todo{should I consider uncertainty on these values?}
This implies a resulting systematic uncertainties introduced due to the scale variance of the simulated data of $\pm 0.03$ for $C_{tG}$ and $^{+0.22}_{-0.23}$ for $C_{tq}^{8}$.

The single observable analysis obtained an estimate for the Wilson coefficients of $C_{tg} = 0.24 \pm 0.11 (stat.) \pm 0.03 (sys.)$ and $C_{tq}^{8}=1.56^{+0.25}_{-0.31} (stat.) ^{+0.22}_{-0.23} (sys.)$.
The model prediction for this estimation can be seen in Figure~\ref{fig:model_result_1D_2OP}.
The predictions from the model at the optimised coefficients appears to better describe the data compared to using the model with all coefficients force to zero.
The optimised prediction corresponded to a $\chi^{2}$ per degree of freedom of 0.54 with respect to the data while the all zero prediction corresponded to a $\chi^{2}$ per degree of freedom of 3.16 indicating that the optimised prediction better describes the data.

When these coefficient estimations are compared with those obtained when using just $C_{tg}$, there is an agreement with the value of $C_{tG}$ when considering both the statistical and systematic uncertainty on these measurements.
However, the estimation of $C_{tq}^{8}$ in the double operator fit deviates greatly from the assumed value of $C_{tq}^{8}=0$ for the single operator models and for SM predictions.

This does provide some interest in possibly applying constraints beyond the Standard Model but there are some faults with this consideration.
The systematic uncertainty associated with the model creation and prediction was not accounted for in the uncertainty estimation.
Since these influences will equally be present when considering the double differential cross section data, this discussion will be deferred to later sections and these results will be used as a point of comparison between single and double observables analyses.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg-ctq8_1D_2OP.png}
        \caption{1-Dimensional Observable}
        \label{fig:corner_1D_2OP}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg-ctq8_2D_2OP.png}
        \caption{2-Dimensional Observable}
        \label{fig:corner_2D_2OP}
    \end{subfigure}
    \centering
    \caption{Estimation for the 2D likelihood distribution and 1D marginal distributions for the Wilson coefficient $C_{tG}$ and $C_{tq}^{8}$ obtained by fitting each model onto the respective ATLAS differential cross section measuremnts. The blue lines represent SM predictions. The dotted lines on the marginal distributions represent a 68\% confidence interval. Uncertainty estimates for the Wilson coefficients are discussed in Section \ref{sec:fitting} and only represent statistical error.}
    \label{fig:corner_2OP}
\end{figure}

\paragraph{Double observable model}

The double observable analysis when considering the two operators provided results which fall more in line with the results found for the single operator case.
The distributions produced through the MCMC method have estimates of the Wilson coefficient values of $C_{tG}=0.49_{-0.07}^{+0.06}$ and $C_{tq}^{8}=-0.51_{-0.70}^{+0.69}$.
It can be seen that the 2D observable model applies better constraints of statistical uncertainty on $O_{tG}$ similar to what occurred for the 1D observable model.
The shape of the distribution shows a more prominent double peak structure compared to what was seen in the single observable analysis for two observables.
This makes interpreting the value of the coefficients fairly difficult. \todo{more detail}

These two peaks provide two optimised values which would maximise the likelihood which isn't fully compatible with the quantile definitions of the best fit value and uncertainty which has been used for evaluating the optimised coefficient values for the other models.
In order to extract the estimations provided by each of these peaks, a more traditional likelihood maximisation algorithm through the L-BFGS algorithm provided through SciPy~\cite{2020SciPy-NMeth} was used.
The two maximums were determined by providing different initial conditions for each peak.
The uncertainty on these estimates were obtained by using the inverse Hessian matrix provided by the algorithm implementation.
The estimation for the points of maximum likelihood are $C_{tq}^{8\,(1)}=-1.14\pm0.39$ and $C_{tq}^{8\,(2)}=0.10\pm0.37$.
This provides a solution $C_{tq}^{8\,(2)}$ which agrees with zero and therefore is consistent with the SM prediction and the single operator model result for the double observable.
Since the SMEFT does allow these multiple peaks, the entire distribution should be considered and the estimation found using quantiles will be considered further.
If more operators were included in some analysis, another coefficient could suppress one of these peaks and provide a clear measurement of $C_{tq}^{8}$.

The estimated systematic uncertainty associated with the scale variance of the simulated samples is $^{+0.11}_{-0.13}$ for $C_{tG}$ and $^{+0.21}_{-0.87}$ for $C_{tq}^{8}$. \todo{maybe change to table}
The final estimation for the Wilson coefficients using the quantile definition for the $t\bar{t}$ differential cross section with respect to $m_{t\bar{t}}$ and $p_{T}^{t}$ are $C_{tG} = 0.49_{-0.07}^{+0.06}(stat) ^{+0.11}_{-0.13} (sys.)$ and $C_{tq}^{8}=-0.51_{-0.70}^{+0.69} (stat.) ^{+0.21}_{-0.87} (sys.)$.
The model prediction for this estimation can be seen in Figure~\ref{fig:model_result_2D_2OP}.
The fitted parameters provide a better goodness of fit with respect to the data compared to SM predictions of the model with the fitted values corresponding to a $\chi^{2}$ per degree of freedom of 2.76 while the SM predictions resulted in a $\chi^{2}$ per degree of freedom of 5.60.

Both of the peak solutions do not agree with the optimal parameter found for the single operator model within statistical uncertainty.
This suggests that that the result for the single observable is describing some qualities of the cross sectional distribution which is not present when the double observable is considered.
This is extremely interesting since it indicates that the method for model creation used can generate models which produce estimations for the coefficients which are not consistent between the single differential cross section and the double differential cross section.
A possible reason for this discrepancy may be due to the modelling of $C_{tq}^{8}$ and the ability for the morphing model creation to describe the operator's relationship to $\frac{\partial \sigma}{\partial m_{t\bar{t}}}$.
The use of a double differential cross section may expose more information needed for the model creation which is integrated out when looking at the single differential.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_1D_2OP.png}
        \caption{1-Dimensional Observable}
        \label{fig:model_result_1D_2OP}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_2D_2OP.png}
        \caption{2-Dimensional Observable}
        \label{fig:model_result_2D_2OP}
    \end{subfigure}
    \centering
    \caption{Comparison of differential cross section of $t\bar{t}$ production between various morphing model predictions and ATLAS data. This includes model predictions for the optimised Wilson coefficients and for the all zero coefficient case (labelled as SM pred.). The ratio between each model prediction and the data is shown underneath. The errors shown represent statistical and systematic uncertainties of the data.}
    \label{fig:model_result_2OP}
\end{figure}

\section{Conclusion}
\begin{itemize}
    \item describe the analyses done
    \item compare the results briefly
    \item mention the increased constraints on $C_{tg}$
    \item difficulties in measuring $C_{tq}^{8}$
    \item describe future work
    \begin{itemize}
        \item performing at NLO
        \item considering more 4 fermion operators
        \item look at $p_{T}^{t}$ distribution
    \end{itemize}
\end{itemize}

\clearpage
\begingroup
\raggedright{}
\sloppy
\printbibliography{}
\endgroup

\end{document}
