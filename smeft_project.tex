\documentclass[a4paper,11pt]{article}

\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[style=numeric-comp,sorting=none]{biblatex}
\usepackage{warning} % for warning messages

\parskip=1.5ex
\parindent=0pt

\usepackage{url}
\usepackage{microtype}
\usepackage{lmodern}
\usepackage[colorlinks=true, citecolor=red, linkcolor=blue]{hyperref}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[affil-it]{authblk}


\usepackage{todonotes}

\usepackage{siunitx}

\usepackage[compat=1.1.0]{tikz-feynman} % generate feynman diagrams

\renewcommand{\vec}{\mathbf}
\newcommand{\ts}{\textsuperscript}

\bibliography{references}
\overfullrule=2cm % allows to find overfull hboxes much quicker

\binoppenalty=3000
\relpenalty=3000

\title{Applications of Standard Model Effective Field Theory to 2D differential distributions of top pair production}
\author{Alexander Veltman\\{\small Advisor: Dr.\ James Keaveney}}
\affil{Department of Physics,\\University of Cape Town}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

The Standard Model (SM) is the best description of nature at the subatomic level that modern physics has to offer.
Using the interactions present in the SM, predictions of observable can be verified to extreme precision using the latest measurements.

\begin{itemize}
    \item Standard model is good and accurate
    \item Cannot explain some phenomena i.e. CP violation and dark energy and dark matter
    \item direct search for BSM at LHC, no new results
    \item either non-coupling to SM or high energy
    \item Consider effective interactions at high energy
    \item New frame work called SMEFT
    \item goal is to compare ability to constrain between 1D and 2D cross sections
\end{itemize}


\section{Top Physics}

The top quark is the most massive particle in the Standard Model with a rest mass of $m_{t} =172.76\pm0.30$GeV~\cite{ParticleDataGroup:2020ssz} and is commonly used as an regime to look for physics beyond the Standard Model.
It holds both electrical charge as well as colour charge which allows it to participate in electroweak and strong interactions.
Due to high luminosity achieved by the various experiments present at the Large Hardron Collider, very detailed cross sectional measurements have been achieved, allowing for the ability to implement differential cross section measurements of top quark production in high energy physics analysis.

Top quarks are usually produce in top-antitop pairs through the $gg\rightarrow t\bar{t}$ and $q\bar{q}\rightarrow t\bar{t}$ processes at leading order.
Leading order (LO) and next-to-leading order (NLO) refer to the number of interactions required for the process to occur.
For example, the process $gg\rightarrow t\bar{t}$ at leading order involves the incoming gluons annihilating into a single gluon which then decays into a top pair with no additional particles being created or annihilated.
At the energy levels of the $pp$ collision at the LHC, about 90\% of $t\bar{t}$ production occurs through $gg\rightarrow t\bar{t}$.
Single top production is also possible through electroweak processes like $q\bar{q}^{\prime}\rightarrow t\bar{b}$ but is not relevant for the observables considered in this report.

Top quarks almost always decay into a $W$ boson and a $b$ quark since it has a short lifetime, the top quark are unable to hadronise.
Due the $t$ quark's larger mass, it is actually the only quark which can decay in this manner.
This channel in which a top pair is characterised is by the manner in which the two W bosons produced by each of the tops decay.
The tops are then classified as decaying leptonically if the $W$ decays into a lepton and an associated neutrino or hadronically if the $W$ decays into 2 quarks.
Any resulting quarks in the final state hardonise to form jets of hadrons which are then deposited into the detectors.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \feynmandiagram[horizontal=a to b] {
            i1 -- [gluon] a -- [gluon] i2,
            a -- [gluon] b,
            f1 [particle=$\bar{t}$] -- [fermion] b -- [fermion] f2 [particle=$t$],
        };
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \feynmandiagram[horizontal=a to b] {
            i1 [particle=$q$] -- [fermion] a -- [fermion] i2 [particle=$\bar{q}$],
            a -- b,
            f1 [particle=$\bar{t}$] -- [fermion] b -- [fermion] f2 [particle=$t$],
        };
    \end{subfigure}
    \caption{Tree level Feynman diagrams for top quark pair production}
\end{figure}


\todo{tops in BSM physics}
\nocite{Thomson:2013zua}
The interest in the top quark in terms of beyond standard model physics is due to it being the quark with the largest coupling to the Higgs boson with a Yakawa coupling $y_{t} \approx 1$.

\section{LHC and the ATLAS experiment}
The Large Hadron Collider (LHC) is a two-ring-superconducting particle accelerator located at the European Organization for Nuclear Research (CERN) and is the centre-piece of proton-proton collision physics.
The design consists of two counter-rotating proton beams in a circular ring with a diameter of \SI{26.7}{\kilo\metre}.
During the years of 2016 to 2018, the maximum achievable centre-of-mass energy $\sqrt{s}$ at the LHC was \SI{13}{\tera\electronvolt} with a luminosity of \si{10^{34}\centi\metre\squared\per\second}.
Upgrades have been performed in recent years to allow energies of \SI{14}{\tera\electronvolt}.~\nocite{Evans:2008zzb}
These energies are the highest to be achieved for a particle collider and has allowed for notable landmarks in particle physics including the discovery and verification of the Higgs Boson.
The LHC is home to a family of detector experiments including ATLAS, ALICE, CMS and LHCb.

The ATLAS collaboration is a large collaboration of physics, engineers, technicians and students with the goal of probing proton-proton collisions using the ATLAS detector.
It is considered, along with the CMS detector, as a general purpose detector which can measure the various different products created during the collisions.

The ATLAS detector consists of various components which together allow for the general detection of particle collisions.
The largest of which is the magnet system.
The magnet system allows for the adjustment of particle tracks in order for the momenta of these particle to be measured within the subdetectors.
The Inner Detector (ID) is the component closest to the beam and cylindrically surrounds the beam pipe.
This layer performs pattern recognition, momentum and vertex measurements as well as electron identification by applying subdetectors.
Further out from the beam, there are the calorimeters which provide electromagnetic and hadronic energy measurements.
These allow for the measurement and identification of photons, electrons, jets and missing transverse energy.
The muon spectrometer forms the outer part of the detector.
High muon momentum resolution is possible due to the three layers of high precision tracking chambers.
There are also three forward detectors which are placed in regions of low azimuthal angles with respect to the beam line.
The LUCID detector was the first implemented and it is designed to be a luminosity monitor for ATLAS.
The Zero-Degree Calorimeter (ZDC) detects forward neutrons in heavy-ion collisions.
The ALFA (absolute luminosity for ATLAS) detector measures elastic proton-proton scattering.
All these components allow for the reconstruction and measurement of the various underlying physical processes.

\section{Standard Model Effective field theory}

\nocite{Zhang2011}
\nocite{franzosi2015probing}

Standard Model effective field theory (SMEFT) is a model-independent framework for identifying and constraining deviations from Standard Model predictions.
This is done by considering that some higher mass particles or higher energy reactions may exist and seeing the imprints on regular Standard Model cross-sections and interactions.
The framework introduces a set of dimension-six terms into the Standard Model Lagrangian which only contains operators of dimension-four.
These includes 59 independent operators (according to what is known as the Warsaw basis) which are built from Standard Model fields and follow the gauge symmetries of the Standard Model \cite{Grzadkowski_2010}.

With these additional operators, the SMEFT Lagrangian is
\begin{equation}\label{eq:smeft_lagrangian}
    \mathcal{L}_{\text{SMEFT}} = \mathcal{L}_{\text{SM}} + \frac{1}{\Lambda^2} \sum\limits_{i} C_{i} O_{i} + \mathcal{O}\left(\frac{1}{\Lambda^3}\right)
\end{equation}
where $O_{i}$ is a dimension-six operator and $C_{i}$ is an associated dimensionless coupling constant known as a \emph{Wilson Coefficient}.
The operators are reduced by the energy scale $\Lambda$ of the BSM physics.
These effects manifest themselves in observable cross sectional data~\cite{Hartland_2019} as
\begin{equation}\label{eq:smeft_cross_section}
    \sigma = \sigma_{\text{SM}} + \sum\limits_{i} \frac{1}{\Lambda^2} C_{i} \sigma_{i} + \sum\limits_{j,k} \frac{1}{\Lambda^4} C_{j} C_{k} \sigma_{j k}
\end{equation}
where $\sigma$ is an integrated cross section. This can also be extended to differential cross sections which is commonly obtained in high energy physics experiments.
It is important to note that when $C_{i}=0$ for all operators, the SMEFT Lagrangians simplifies to the SM Lagrangian.
This implies that a sufficient deviation from a zero measurement may imply affects of new physics.

The effects on differential cross section with respect to an observable $X$ are similar,
\begin{equation}\label{eq:smeft_diff_cross_section}
    \frac{d\sigma}{dX} = \frac{d\sigma_{\text{SM}}}{dX} + \sum\limits_{i} \frac{1}{\Lambda^2} C_{i} \frac{d\sigma_{i}}{dX} + \sum\limits_{j,k} \frac{1}{\Lambda^4} C_{j} C_{k} \frac{d\sigma_{j k}}{dX}
\end{equation}
in which these differentials are presented as binned measurements in data.

Using (\ref{eq:smeft_diff_cross_section}), the influences of SMEFT can be identified within differential cross section measurements obtained through modern collider experiments.
Typically, this is done using global fits to many differential cross sectional measurements with respect to different observables.
This allows different operators, which may be coupled to some observables more than others, to be more effectively constrained.

There has been interest in looking at the influences of SMEFT within the study of top quarks~\cite{Hartland_2019,Buckley_2015,Brivio_2020} due to the possibility of the top quark as an area for possible BSM physics.
This report will investigate top pair production whose cross section at leading order is only impacted by limited sets of the dimension-6 operators. For the $q\bar{q} \rightarrow t\bar{t}$ process, the only relevant operator is the chromomagnetic moment operator, $O_{tg}$.
The $gg \rightarrow t\bar{t}$ process is affected by $O_{tg}$ as well as a set of 8 operators known as four-fermion operators.
This report will include the four-fermion operator $O_{tq}^{8}$.

The explicit form of these operators are
\begin{align*}
    O_{tG} = (\bar{Q} \sigma^{\mu \nu} T^{A} t) \tilde{\phi} G^{A}_{\nu \mu} && O_{tq}^{8} = (\bar{q}_{i} \gamma^{\mu} T^{A} q_{i})(\bar{t} \gamma_{\mu} T^{A} t)
\end{align*}
where these fields expose some of the interactions which will be present in $t\bar{t}$ production.
The operator $O_{tG}$ introduces or modifies the interactions seen in Figure~\ref{fig:smeft_diagrams} and the four fermion operator $C_{tq}^{8}$ introduces an effective contact vertex of $q\bar{q} \rightarrow t\bar{t}$.
Some of these are modifications of SM vertices while some are completely new effective interactions which are not present in the SM.



\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \feynmandiagram [small, horizontal=a to b] {
            i1 [particle=$\bar{q}$] -- [fermion] a -- [gluon] b [dot] -- [fermion] f1 [particle=$t$],
            i2 [particle=$q$] -- [anti fermion] a,
            b   -- [anti fermion] f2 [particle=$\bar{t}$],
        };
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \feynmandiagram [small, vertical=a to b] {
            i1 [particle=$g$] -- [gluon] a [dot] -- [fermion] f1 [particle=$t$],
            i2 [particle=$g$] -- [gluon] b [dot] -- [anti fermion] f2 [particle=$\bar{t}$],
            b -- [fermion] a,
        };
    \end{subfigure}
    %\hfill
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{feynman}
                \vertex (a);
                \vertex [above left=1cm of a] (i1) {$g$};
                \vertex [above right=1cm of a] (f1) {$t$};
                \vertex [below left=1cm of a] (i2) {$g$};
                \vertex [below right=1cm of a] (f2) {$\bar{t}$};
            \diagram* {
                (i1) -- [gluon] (a) [dot] -- [fermion] (f1),
                (i2) -- [gluon] (a) -- [anti fermion] (f2),
            };
            \end{feynman}
        \end{tikzpicture}
    \end{subfigure}

    \caption{Examples of leading order diagrams which contribute to top pair production in SMEFT}
    \label{fig:smeft_diagrams}
\end{figure}




\section{dEFT}

dEFT, or differential Effective Field Theory tool, is an Python package created by Dr. James Keaveney~\cite{Keaveney_dEFT} to allow for predictions of SMEFT effects using differential cross section measurements.
For this report, the repository was forked and further development was performed.
The version used for the analysis contained in this report is available through Github~\cite{codecalec_dEFT} or the PyPI repositories~\cite{pypi_dEFT}.
Using a single configuration file containing both data and Monte Carlo predictions, dEFT can build a predictive morphing model which is used to estimate a posterior distribution for the relevant Wilson coefficients.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{images/deft-workflow.pdf}
    \caption{Overview of dEFT workflow}
\end{figure}

\subsection{Model building}
dEFT creates predictions for the observables of interest for varying values of Wilson coefficients by constructing a linear morphing model.
The morphing model is a linear regression model which allows for the interpolation between different templates.
These templates are Monte Carlo predictions which are generated around the region of parameter space which would be relevant for some dataset.
For dEFT's application, cross sectional SMEFT predictions which are generated using a event generation framework are treated as templates.
These predictions must describe the relevant set of operators $O_{i}$ with varying Wilson Coefficients in order to produce a reliable model.
Using ($\ref{eq:smeft_cross_section}$), a linear model is constructed using these templates and produces a predictive model $\hat{\sigma}({C_i})$.
This model now allows for the prediction of some cross sectional observable for any values of Wilson coefficients pertaining to the relevant set of operators.
Due to the reliance on MC samples, the model is dependent on the ability to produce high quality SMEFT differential cross section predictions.

The model must be validated to ensure sensible predictions are being performed and the model predictions can be considered reliable.
Additional Monte Carlo samples are generated at coefficient values between the templates used to create the model.
These differential cross section samples are then compared to the model predictions to ensure the predictions agree within a suitable error comparable to the statistical error on the Monte Carlo samples.
Unfortunately, due to validating using Monte Carlo samples, this method of validation is still vulnerable to issues which may arise from the modelling of the samples themselves.

\subsection{Fitting Method}\label{sec:fitting}
Once a model has been built, a fit to data is possible.
dEFT performs fitting using Monte Carlo Markov Chain (MCMC) methods which allow for estimation of the likelihood distributions of the Wilson Coefficients using prior assumptions about their possible values.
The fitting procedure uses \emph{emcee}~\cite{Foreman_Mackey_2013} as its MCMC implementation.
MCMC requires an estimation for the likelihood function $P(y | C_{i})$ which represents the probability of obtaining some data $y$ given a set of model parameters $C_{i}$.

A common log likelihood definition for binned data with Gaussian errors with the associated model $f$ is
\begin{equation}\label{eq:likelihood}
    P(y | C_{i}) \propto \ln\mathcal{L}(y | C_{i}) = -\sum\limits_{n} (y_{n} - f_{n}(C_{i})) V^{-1} (y_{n} - f_{n}(C_{i}))
\end{equation}
where $y_{n}$ is the binned cross sectional data, $V$ is the associated covariance matrix and $f_{n}(C_{i})$ is the morphing model prediction.
In order for an estimation for the posterior likelihood distribution $P(C_{i} | y)$ to be made, a prior distribution $P(C_{i})$ is required.
This takes the form of uniform distributions defined by some minimum and maximum for each $C_{i}$ parameter.
MCMC will systematically sample throughout $C_{i}$ space building an estimation for the posterior distribution $P(C_{i}|y)$.
Properties regarding $C_{i}$ can then be inferred.

Since MCMC methods are used, an approximations for the $C_{i}$ distributions is obtained rather than a single value with an associated uncertainty which is common from other likelihood maximisation methods.
This avoids the issue of finding a local maximisation which can be common due to the quadratic nature of the SMEFT model.

The estimation for the coefficient is extracted from the likelihood distributions by considering percentiles of the discrete MCMC sampler prediction of the marginalised distribution of each operator.
The 50\ts{th} percentile is attributed as the estimation for the coefficient with the 16\ts{th} and 84\ts{th} percentile forming a 68\% confidence interval about the estimate.
This estimate allows for asymmetrical errors which can describe the measurement more faithfully than considering just the mean and variance of the samples.

\todo{talk about Smefit methods}

\section{Analysis}
This analysis will examine the possibility of using double differential cross section measurements with respect to multiple in a SMEFT analysis.
The results will be compared to outcomes when considering a single differential cross section.
The main aim of a SMEFT analysis is to place constraints onto Wilson coefficients of SMEFT operators allowing us to investigate the potential occurrences of new interactions or modifications to SM interaction.
Double differential cross sections are of interest due to the possibility of simultaneously constraining multiple operators which may present as different modifications to observable cross section distributions.

The only SMEFT operators considered were $O_{tG}$ and $O_{tq}^{8}$ with corresponding Wilson coefficients $C_{tG}$ and $C_{tq}^8$.
For a full phenomelogical study, the entire set of relevant operators should be included but this would drastically increase the scale of computational processing.
The limited set of operators should still show the influences of changing from a single dimensional differential cross section to a multidimensional differential cross section.

This analysis will begin with the details of discussion of the ATLAS datasets and the Monte Carlo event generation needed to create the morphing models for performing the constraints on the Wilson coefficients.
This will move into applying this model and obtaining estimations for the distribution of the coefficients for both the single observable and the double observable.

\subsection{Discussion of Datasets}
This report will use differential cross section data of top pair production from the ATLAS experiment~\cite{ATLAS:2019hxz} at the CERN Large Hadron Collider.
The data used is publicly available through HEPData~\cite{hepdata1750330}.
This data was produced from pp collisions performed at a centre-of-mass energy $\sqrt{s} = 13$TeV over the course of 2015 and 2016 with an integrated luminosity of 36.1fb$^{-1}$.
The $t\bar{t}$ final states are extracted from the $\ell$+jets channel in the resolved topology.
Resolved topology implies that the decay products of the hadronically decaying top quark are angularly well separated.

The double differential cross section observable considered  were the differential cross section as a function of the invariant mass of the $t\bar{t}$ system $m_{t\bar{t}}$ and the transverse momentum of the hadronically decaying top quark $p_{T}^{t}$.
These are the mass of the top-antitop system in it's rest frame and the component of the hadronically decaying top's momentum travelling radially from the beam line in the collider.
For the comparison with a single observable, the differential cross section as a function of just $m_{t\bar{t}}$ was examined.
The differential cross section data for just $m_{t\bar{t}}$ may be referred to as the single or 1D obervable and the differential cross section data with respect to $m_{t\bar{t}}$ and $p_{T}^{t}$ may be referred to as the double or 2D observable.

The data provided contains the associated statistical and systematic uncertainties introduced during the measurements of the cross sections.
Multiple sources of systematic uncertainty are introduced in the process of measuring the $t\bar{t}$ systems.
This includes the uncertainty associated with the reconstruction of the lepton and the various jet products of the process.
There are also the uncertainty associated with the tagging of jets containing $b$-hadrons as well as accounting for possible misidentifications of jet flavour.
The uncertainty associated with the reconstruction of the missing transverse momentum $E_{T}^{\text{miss}}$ must also be accounted for.
Error in the modelling of the signal and background contributions and the Monte Carlo statistical error involved in creating these models are propagated.

Through HEPData, the covariance matrices associated with the differential cross section measurements are made available to the public.
The analysis was initially conducted including the full covariance matrix of each data set which was implemented into the likelihood definition as seen in (\ref{eq:likelihood}).
Unfortunately, this caused the likelihood definition to produce fits which described the data very poorly compared to if only the diagonals of the matrices were considered.
For the differential cross section data with respect to $m_{t\bar{t}}$, the fit produced with the full covariance matrix had a corresponding $\chi^2$ per degree of freedom of 12.60 while this was reduce to 0.53 when only considering the diagonal elements.
The covariance matrices provided were examined and it was found that they indicated that each bin in the histogram was highly correlated for a dataset of this type.
The single differential cross section histogram is shown as an example in Figure~\ref{fig:correlation}.
This was considered to be an error in the covariance matrix provided by HEPData and the covariance between different bins is ignored.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/correlation.png}
    \caption{Correlation between $m_{t\bar{t}}$ bins of histogram for the differential cross section with respect to $m_{t\bar{t}}$ dataset}
    \label{fig:correlation}
\end{figure}

\subsection{Monte Carlo event generation}
In order to build the morphing model required to generate cross sectional predictions, simulated samples are required throughout the space of Wilson coefficients of the operators of interest.
These samples were generate using the MadGraph5\textunderscore aMC@NLO~\cite{Alwall_2014} framework which allows for the simulation of processes for a user-defined Lagrangian.
The SMEFTatNLO~\cite{degrande2020automated} FEYNRULES model implements SMEFT tree level and one loop processes into MadGraph5.
Though there is the capacity to perform predictions at next-to-leading order, these calculations are very recent \todo{Justify why LO better} and greatly increase the processing time required to produce the Monte Carlo predictions.
The simulations are performed to fixed order where only the desired observables of $m_{t\bar{t}}$ and ${p_{T}^{t}}$ are calculated and binned in the same binning arrangement as the ATLAS dataset of interest.

Separate sets of MC signal was required for the single observable and the double observables analysis.
Events were generated with values of $C_{tg}$ as -4, -3, -2.5, -2, -1, -0.5, 0, 0.5, 1, 2, 2.5, 3 and 4 and values of $C_{tq}^{8}$ as -4, -3, -2.5, -2, -1, -0.5, 0, 0.5, 1, 2, 2.5, 3 and 4. \todo{maybe rephrase this}
All permutations of these values were used resulting in a total number of 169 samples to build the two operator model and 13 samples to build the single operator model.
The two operator model validation involved generating 100 validation samples at points different to those used for the building of the model.
This simulation step imposes a great challenge when wanting to expand into using more SMEFT operators in the model.
The number of required MC samples to ensure consistent description of the parameter space increases exponentially with the number of operators.
This is a major factor in the decision to not include all 4-fermion operators in this report and would require more time and computational capacity.
\todo{mention this fact in conclusion}

\subsubsection{Uncertainty due to simulation}\label{sec:scale_variance}

The simulation calculations were required to obtain a statistical accuracy of 1\% for the prediction of the integrated cross section of top-antitop production.
This level was considered a reliable since this error was minimal in comparison to the total uncertainty of the cross sectional data but still able to be performed in a reasonable time frame.
An accuracy requirement on each bin was unavailable and would have allowed for a clearer comparison to the error associated with the data.

Theoretical uncertainties on the Monte Carlo samples also needs to be accounted for.
This was done by varying the renormalisation and factorisation scales by two for the upper scale and by a half for the lower scale.
The scale variance was calculated through MadGraph5 using the SMEFTatNLO model with all operators being suppress.
This provided a scale variance prediction for the Standard Model at NLO which could then be applied to SMEFT samples.
\todo{talk about error compared to validation value}

\subsubsection{Calculation of \texorpdfstring{$k$}{k}-factor}
Since the generated Monte Carlo samples only included LO processes, these predictions needed to be scaled to be comparable to the necessary data sets.
It can be considered fairly accurate to compare NNLO predictions to actual cross sectional data so a method of scaling the current predictions to this level is required.
Due to the difficulty in calculating the $k$-factor for different combinations of Wilson coefficients values, the $k$-factor for the Standard Model prediction was used across the various SMEFT predictions.

A flat $k$-factor across the differential cross sections was attempted to bring the LO predictions to the scale of NNLO predictions using the proportions between tree level calculations of total cross section~\cite{Alwall_2014} and measurement of total cross section using the ATLAS detector~\cite{ATLAS:2019hxz}.
This method failed due to some regions of the differential cross sections being poorly described at LO which caused a flat scaling to incorrectly describe the shape of the distributions at NNLO.
This is exemplified by the low $m_{t\bar{t}}$ and high $p_{T}^{t}$ region in our two observable data set, as seen in Figure~\ref{fig:kfactor}.
This is remedied by requiring a per-bin $k$-factor when scaling from LO to NLO but still using the flat factor to build up to NNLO.
The per-bin $k$-factor was found by comparing the Standard Model predictions of MadGraph5 at LO and NLO and applying this ratio to the Monte Carlo signal.
The theoretical error associated with the calculated $k$-factor is fairly difficult to propagate through the model construction and should be dominated by the error associated with the data.
This may have consequences on bins which are not well described at LO though the scale variances in these regions should dominate the uncertainties introduced by the $k$-factor.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/k_factor.png}
    \caption{Comparison of estimations of absolute double differential $t\bar{t}$ cross section with respect to $m_{t\bar{t}}$ and $p_{T}^{t}$ to varying orders. The scaling from an LO approximation to an NLO approximation was determined using a per-bin method and the NNLO prediction was determined using a flat factor.}
    \label{fig:kfactor}
\end{figure}

\subsubsection{Response to operators}\label{sec:response}

With a set of observables declared, Monte Carlo simulated events can be used to examine the response of $t\bar{t}$ differential cross sections when the associated Wilson coefficients are varied in the theory.
Various theoretical predictions are shown in Figure~\ref{fig:comparison} for the double differential cross section explored in this report.
Focussing on the response with respect to $C_{tG}$, there appears be a general scaling effect on the majority of bins in distribution.
This provides a clear interpretation of the operator but exposes some issues in constraining it.
If $O_{tG}$ is related to a scaling of the distribution, any other contribution which may scale the theoretical predictions may impact the estimation of $C_{tG}$ such as the $k$-factor used to ensure the leading order MC predictions are comparable to the data.
It is assumed that any effects due to not describing even higher order than NNLO or error in the $k$-factor estimation would affect the distribution uniformly.
This implies that the size of constraints determined for $C_{tG}$ would not be effected if the scale of the MC samples were adjusted.

The operator $C_{tq}^{8}$ has a more varied response across the distribution.
It appears that the affects of $O_{tq}^{8}$ are more prominent at the higher regions of transverse momentum of the hadronically decaying top.
A major difference between the response in $C_{tq}^{8}$ and $C_{tG}$ is that an increasing $C_{tq}^{8}$ does not indicate an increase of cross section in a bin.
This non-uniformity in how the bins transform could lead to distributions which aren't necessarily Gaussian.
If these effects are considered when modelling, it is expected that $C_{tq}^{8}$ may experience multiple optimised values which will be seen as a distribution with multiple peaks.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/data_plot_ctg.png}
        \caption{Response to $C_{tG}$}
    \end{subfigure}

    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/data_plot_ctq.png}
        \caption{Response to $C_{tq}^{8}$}
    \end{subfigure}
    \caption{Comparison of simulated theoretical predictions of double differential cross section of $t\bar{t}$ production as a function of $m_{t\bar{t}}$ and $p_{t}^{T}$ for varying values of $C_{tG}$ and $C_{tq}^{8}$. The ATLAS measurements are also shown.}
    \label{fig:comparison}
\end{figure}

\subsection{Single operator model}

As a test of method, a model was constructed considering only the $O_{tG}$ operator and applied to the 1-dimensional and 2-dimensional differential cross section measurements.
Sensible results for this limited analysis will show the ability for dEFT to generate a successful model using the Monte Carlo samples and be able to link connections between the two datasets which share an influence from the distribution of cross section with respect to the mass of the top quark pairs.
A separate model was created to suite each of the datasets which required separate sets of Monte Carlo samples in which the value of $C_{tG}$ was varied and the value of $C_{tq}^{8}$ was suppressed.

\subsubsection{Model Validation}

The models used were created using Monte Carlo samples which influenced by statistical uncertainty.
For the model to be considered suitable, the model should provide predictions of other Monte Carlo samples with a similar degree of certainty.
This deviation is expected to be comparable to the 1\% required accuracy for integrated cross section imposed on the event generation within MadGraph5.


Only 10 validation points of varying $C_{tG}$ were used for the 1-dimensional cross section and the 2-dimensional models.
In order to compare the validation tests to the statistical simulation accuracy, the average relative residuals between the validation samples and the models' prediction for the associated coefficient values were considered.
This is just the summation of the relative residuals of each bin in each test divided by the total number of bins across all tests.
The standard deviation of this value should be in-line with the required accuracy.

An example of one of these tests can be seen in Figure~\ref{fig:test_example}. All predictions across the bins in the tests are within at least 5\% of the MC sample provided by MadGraph5.
For most of the tests, the deviations per bin do exceed the 1\% accuracy required for the integrated cross section on some of the bins.
However, considering the average relative residuals allows for a more general estimation of the performance of the models predictive capacity.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\textwidth]{plots/validation_ATLAS-ctg_2_1D_1OP.png}
    \caption{Example of validation test for single observable model for $C_{tG}=2$. The error bars shown represent a 1\% deviation on the MadGraph5 prediction.}
    \label{fig:test_example}
\end{figure}

The average relative residuals with the associated standard deviation is $0.10\pm1.21\%$ for the 1D observable model and $0.35\pm1.30\%$ for the 2D observable model.
The results of the validation tests show that both models on average provide very small deviation compared to the Monte Carlo signal and is comparable to the uncertainty introduced by the statistical error associated with the Monte Carlo signal.

\subsubsection{Uncertainty due to scale variance}
The theoretical uncertainty related to the Monte Carlo samples are encapsulated by the upper and lower bound of the scale variance as mentioned in Section~\ref{sec:scale_variance}.
This uncertainty is propagated through the model by reperforming the fitting procedure for when the Monte Carlo samples are adjusted to the upper and lower scales.
This will provide two sets of results for the coefficients which can be used as bounds to estimate the systematic uncertainty introduced by the theoretical variance in the simulated samples.

The estimated uncertainty associated with the scale variance of the Monte Carlo samples for the 1-dimensional model is $^{+0.07}_{-0.06}$ and for the 2-dimensional model is $^{+0.16}_{-0.16}$.

\subsubsection{Results}
The estimated likelihood distributions created during the fitting procedure for the 1-dimensional observable and the 2-dimensional observable are shown in Figure~\ref{fig:corner_1OP}.
Both observable types produced likelihood distributions with a similar Gaussian shape.
The estimation for the Wilson coefficient using the single differential cross section was $C_{tG}=0.46^{+0.08}_{-0.09}(stat.)^{+0.07}_{-0.06}(sys.)$ and using the double differential cross section, the obtained estimate was $C_{tG}=0.47^{+0.05}_{-0.05}(stat.)^{+0.16}_{-0.16}(sys.)$.
The resulting prediction for the single and double differential cross section models is shown in Figure~\ref{fig:model_result_1OP}.
These predictions show that the optimised estimate for $C_{tG}$ provides better prediction of the data for both models then the SM prediction of the models.
The $\chi^{2}$ per degree of freedom statistic for these predictions with respect to their data quantify the goodness of fit.
The 1D observable model predictions has an associated $\chi^{2}$ per degree of freedom of 1.2 for the optimised prediction and 3.16 for the SM prediction.
For the 2D observable model, the $\chi^{2}$ per degree of freedom was 2.53 for the optimised prediction and 5.60 for the SM prediction.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg_1D_1OP.png}
        \caption{1-Dimensional Observable}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg_2D_1OP.png}
        \caption{2-Dimensional Observable}
    \end{subfigure}
    \caption{Estimation for the likelihood distribution for the Wilson coefficient $C_{tG}$. The dotted lines on distributions represent a 68\% confidence interval. Uncertainty estimates for the Wilson coefficients are discussed in Section \ref{sec:fitting} and only represent statistical error.}
    \label{fig:corner_1OP}
\end{figure}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_1D_1OP.png}
        \caption{1-Dimensional observable model}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_2D_1OP.png}
        \caption{2-Dimensional observable model}
    \end{subfigure}
    \caption{Predictions of $m_{t\bar{t}}$ differential cross section measurements using two models at the optimised value of the model parameter with respect to the respective ATLAS datasets. The data used to perform the fit and the models' predictions for when all SMEFT operators supressed are also shown.}
    \label{fig:model_result_1OP}
\end{figure}


The estimation for the optimised value of $C_{tG}$ for the models agree within a 68\% confidence interval and provides some interesting points of comparison.
The statistical uncertainty associated with the measurement is lower for the double differential cross section model.
This means that even though there are increased relative errors on the bins for double differential cross sections measurements then for single differential cross sections, the double observable model provides greater constraints on $C_{tG}$ than the single observable model.
This may indicate that using this double observable would allow for better constraints on Wilson coefficients in SMEFT analyses.
A caveat arises when the systematic uncertainty is considered.
The systematic uncertainty is larger when using the double observable which may indicate that the bins which are allowing for the greater constraint of the coefficient may be more affected by the scale variance.
This indicates that this increase in the ability to constrain the coefficient may expose regions which are not well modelled by the theoretical predictions.


\subsection{Double operator model}

The software and techniques for the single operator analysis could then be used to introduce another operator $O_{tq}^{8}$ as well as $O_{tG}$.
This additional operator increases the scale of the analysis since more Monte Carlo samples are required to sufficiently describe this now 2-dimensional parameter space of the models.
The estimation of likelihood distributions of the coefficients produced by the MCMC methods are now 2-dimensional and expose any correlations between the fitted parameters.

\subsubsection{Model Validation}

The results of the validation testing for both models can be seen in Figure~\ref{fig:residuals_hist} where deviations between the model predictions and the validation samples are shown by the average relative residuals.
The tests for the single observable model appear to follow an accuracy of 1.09\% which is in-line with the expectation from the event generation.
The average relative residuals for the double observable model appear to be distributed by a normal distribution as well but with a deviation of 1.68\%.
This result is larger than the validation deviation of the single obervable but is still comparable to the statistical accuracy of the MC samples.
When considering the results across all validation tests, both models appeared to provide sufficient agreement with the MadGraph5 samples and were used for estimating the distributions of the Wilson coefficients.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./plots/residuals_hist_1D.png}
        \caption{1-Dimensional Observable}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./plots/residuals_hist_2D.png}
        \caption{2-Dimensional Observable}
    \end{subfigure}
    \caption{Distribution of average relative residuals between model predictions at some set of coefficient values and the corresponding MC validation sample for each model. This includes the validation tests over 100 different validation points. The dotted line represents a Gaussian fit to the histogram through $\chi^2$ minimisation with the results of the fit shown.}
    \label{fig:residuals_hist}
\end{figure}

\todo{maybe include example validation plots}
\subsubsection{Results}

The estimations for the 2D likelihood distributions for the Wilson coefficients generated by each model are shown in Figure~\ref{fig:corner_2OP}.
It should be noted that the expansion of the parameter space to 2 operators has allowed for the description of the distribution in the form of a 2D likelihood distribution as well as a marginal distribution for each coefficient.
First, the results of the single observable analysis using two operators will be detailed.

\paragraph{Single observable model}

The distribution created using the single observable model provided estimates of $C_{tG}=0.24^{+0.11}_{-0.11}$ and $C_{tq}^{8}=1.56^{+0.25}_{-0.31}$ with these estimates only considering statistical deviation of the distribution.
A notable features of the marginal $C_{tq}^{8}$ distribution is a large peak at $C_{tq}^{8}\approx-2.5$ second less prominent peak located at $C_{tq}^{8}\approx-2.5$.
This is a product of the quadratic nature of the SMEFT model and is a behaviour which is expected when considering the response of the differential cross section with respect to $m_{t\bar{t}}$ as discussed in section~\ref{sec:response}.
The major peak in the distribution appears to show a negative correlation between $C_{tg}$ and $C_{tq}^8$.
This could be a result of the overall scaling introduced by $O_{tg}$ correlating with the scaling on some bins caused $O_{tq}^{8}$.

The model created using the upper bound of the scale variance of the MC signal produced an estimation of $C_{tG}=0.21$ and $C_{tq}=1.33$.
The lower bound of the scale variance of the MC signal produced an estimation of $C_{tG}=0.27$ and $C_{tq}=1.78$.
\todo{should I consider uncertainty on these values?}
This implies a resulting systematic uncertainties introduced due to the scale variance of the simulated data of $\pm 0.03$ for $C_{tG}$ and $^{+0.22}_{-0.23}$ for $C_{tq}^{8}$.

The single observable analysis obtained an estimate for the Wilson coefficients of $C_{tg} = 0.24 \pm 0.11 (stat.) \pm 0.03 (sys.)$ and $C_{tq}^{8}=1.56^{+0.25}_{-0.31} (stat.) ^{+0.22}_{-0.23} (sys.)$.
The model prediction for this estimation can be seen in Figure~\ref{fig:model_result_1D_2OP}.
The predictions from the model at the optimised coefficients appears to better describe the data compared to using the model with all coefficients force to zero.
The optimised prediction corresponded to a $\chi^{2}$ per degree of freedom of 0.54 with respect to the data while the all zero prediction corresponded to a $\chi^{2}$ per degree of freedom of 3.16 indicating that the optimised prediction better describes the data.

When these coefficient estimations are compared with those obtained when using just $C_{tg}$, there is an agreement with the value of $C_{tG}$ when considering both the statistical and systematic uncertainty on these measurements.
However, the estimation of $C_{tq}^{8}$ in the double operator fit deviates greatly from the assumed value of $C_{tq}^{8}=0$ for the single operator models and for SM predictions.

This does provide some interest in possibly applying constraints beyond the Standard Model but there are some faults with this consideration.
The systematic uncertainty associated with the model creation and prediction was not accounted for in the uncertainty estimation.
Since these influences will equally be present when considering the double differential cross section data, this discussion will be deferred for later\todo{do the later} and these results will be used as a point of comparison between single and double observables analyses.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg-ctq8_1D_2OP.png}
        \caption{1-Dimensional Observable}
        \label{fig:corner_1D_2OP}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS-ctg-ctq8_2D_2OP.png}
        \caption{2-Dimensional Observable}
        \label{fig:corner_2D_2OP}
    \end{subfigure}
    \centering
    \caption{Estimation for the 2D likelihood distribution and 1D marginal distributions for the Wilson coefficient $C_{tG}$ and $C_{tq}^{8}$ obtained by fitting each model onto the respective ATLAS differential cross section measuremnts. The blue lines represent SM predictions. The dotted lines on the marginal distributions represent a 68\% confidence interval. Uncertainty estimates for the Wilson coefficients are discussed in Section \ref{sec:fitting} and only represent statistical error.}
    \label{fig:corner_2OP}
\end{figure}

\paragraph{Double observable model}

The double observable analysis when considering the two operators provided results which fall more in line with the results found for the single operator case.
The distributions produced through the MCMC method have estimates of the Wilson coefficient values of $C_{tG}=0.49_{-0.07}^{+0.06}$ and $C_{tq}^{8}=-0.51_{-0.70}^{+0.69}$.
It can be seen that the 2D observable model applies better constraints of statistical uncertainty on $O_{tG}$ similar to what occurred for the 1D observable model.
The shape of the distribution shows a more prominent double peak structure compared to what was seen in the single observable analysis for two observables.
This makes interpreting the value of the coefficients fairly difficult. \todo{more detail}

These two peaks provide two optimised values which would maximise the likelihood which isn't fully compatible with the quantile definitions of the best fit value and uncertainty which has been used for evaluating the optimised coefficient values for the other models.
In order to extract the estimations provided by each of these peaks, a more traditional likelihood maximisation algorithm through the L-BFGS algorithm provided through SciPy~\cite{2020SciPy-NMeth} was used.
The two maximums were determined by providing different initial conditions for each peak.
The uncertainty on these estimates were obtained by using the inverse Hessian matrix provided by the algorithm implementation.
The estimation for the points of maximum likelihood are $C_{tq}^{8\,(1)}=-1.14\pm0.39$ and $C_{tq}^{8\,(2)}=0.10\pm0.37$.
This provides a solution $C_{tq}^{8\,(2)}$ which agrees with zero and therefore is consistent with the SM prediction and the single operator model result for the double observable.
Since the SMEFT does allow these multiple peaks, the entire distribution should be considered and the estimation found using quantiles will be considered further.
If more operators were included in some analysis, another coefficient could suppress one of these peaks and provide a clear measurement of $C_{tq}^{8}$.

The estimated systematic uncertainty associated with the scale variance of the simulated samples is $^{+0.11}_{-0.13}$ for $C_{tG}$ and $^{+0.21}_{-0.87}$ for $C_{tq}^{8}$. \todo{maybe change to table}
The final estimation for the Wilson coefficients using the quantile definition for the $t\bar{t}$ differential cross section with respect to $m_{t\bar{t}}$ and $p_{T}^{t}$ are $C_{tG} = 0.49_{-0.07}^{+0.06}(stat) ^{+0.11}_{-0.13} (sys.)$ and $C_{tq}^{8}=-0.51_{-0.70}^{+0.69} (stat.) ^{+0.21}_{-0.87} (sys.)$.
The model prediction for this estimation can be seen in Figure~\ref{fig:model_result_2D_2OP}.
The fitted parameters provide a better goodness of fit with respect to the data compared to SM predictions of the model with the fitted values corresponding to a $\chi^{2}$ per degree of freedom of 2.76 while the SM predictions resulted in a $\chi^{2}$ per degree of freedom of 5.60.

Both of the peak solutions do not agree with the optimal parameter found for the single operator model within statistical uncertainty.
This suggests that that the result for the single observable is describing some qualities of the cross sectional distribution which is not present when the double observable is considered.
This is extremely interesting since it indicates that the method for model creation used can generate models which produce estimations for the coefficients which are not consistent between the single differential cross section and the double differential cross section.
A possible reason for this discrepancy may be due to the modelling of $C_{tq}^{8}$ and the ability for the morphing model creation to describe the operator's relationship to $\frac{\partial \sigma}{\partial m_{t\bar{t}}}$.
The use of a double differential cross section may expose more information needed for the model creation which is integrated out when looking at the single differential.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_1D_2OP.png}
        \caption{1-Dimensional Observable}
        \label{fig:model_result_1D_2OP}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ATLAS_model_result_2D_2OP.png}
        \caption{2-Dimensional Observable}
        \label{fig:model_result_2D_2OP}
    \end{subfigure}
    \centering
    \caption{Comparison of differential cross section of $t\bar{t}$ production between various morphing model predictions and ATLAS data. This includes model predictions for the optimised Wilson coefficients and for the all zero coefficient case (labelled as SM pred.). The ratio between each model prediction and the data is shown underneath. The errors shown represent statistical and systematic uncertainties of the data.}
    \label{fig:model_result_2OP}
\end{figure}

\section{Conclusion}
\begin{itemize}
    \item describe the analyses done
    \item compare the results briefly
    \item mention the increased constraints on $C_{tg}$
    \item difficulties in measuring $C_{tq}^{8}$
    \item describe future work
    \begin{itemize}
        \item performing at NLO
        \item considering more 4 fermion operators
        \item look at $p_{T}^{t}$ distribution
    \end{itemize}
\end{itemize}

\clearpage
\begingroup
\raggedright{}
\sloppy
\printbibliography{}
\endgroup

\end{document}
